Customer Churn Analysis

Goal: This project investigates customer churn in an e-commerce dataset, applying exploratory data analysis (EDA), outlier/anomaly detection, feature engineering, and predictive modeling to identify which factors drive churn and to build a reproducible pipeline for churn risk scoring.
-----------------------
Motivation

Customer churn directly impacts business sustainability: acquiring a new customer can cost 5‚Äì7x more than retaining an existing one.
By understanding the drivers of churn, companies can take proactive actions ‚Äî targeted offers, loyalty programs, or personalized engagement ‚Äî to reduce churn and increase lifetime value.
------------------------
Dataset

File: data/raw/E_Commerce_Dataset.xlsx (sheet: "E Comm")

Records: 5,630 customers

Target: Churn (1 = churned, 0 = retained)

Examples of features:

Numeric: Tenure, HourSpendOnApp, OrderCount, CashbackAmount, SatisfactionScore, DaySinceLastOrder, CouponUsed, OrderAmountHikeFromlastYear

Categorical: Gender, MaritalStatus, PreferredPaymentMode, PreferredLoginDevice, CityTier, PreferedOrderCat, Complain
----------------------
Exploratory Data Analysis (EDA)

Target distribution: ~16% churn rate ‚Üí dataset is moderately imbalanced.

Numeric features:

Strong skew in OrderCount and CouponUsed (few customers with very high values).

CashbackAmount shows clusters around promotion thresholds.

Categorical features:

Payment method and preferred order category show clear churn differences.

Customers who complained are more likely to churn.

Outliers:

Univariate IQR flagged outliers in most numeric variables.

Multivariate IsolationForest (~1% contamination) highlighted unusual profiles.

EDA notebooks and interactive Plotly visualizations are available in notebooks/01_eda.ipynb.
------------------------------------------
Feature Engineering

Created domain-inspired features to enrich the dataset:

orders_per_month = OrderCount / (Tenure + 1)

engagement_rate = HourSpendOnApp / (Tenure + 1)

coupon_rate = CouponUsed / (OrderCount + 1)

cashback_per_order = CashbackAmount / (OrderCount + 1)

high_growth_flag = 1{OrderAmountHikeFromlastYear ‚â• 18}

Bands:

Recency: 0‚Äì2, 3‚Äì7, 8‚Äì14, 15+ days

Tenure: 0‚Äì3, 4‚Äì12, 13‚Äì24, 25+ months

All preprocessing is packaged in a scikit-learn Pipeline:

Numeric ‚Üí Median imputation + RobustScaler

Categorical ‚Üí Constant imputation + OneHotEncoder

Saved as artifacts/feature_pipeline.joblib for reproducibility.

Implemented in notebooks/02_feature_engineering.ipynb.
--------------------------------------------------------
Modeling & Results

Baseline: Logistic Regression

Tree-based models: Random Forest, XGBoost, LightGBM

Validation: Stratified train/test split, with ROC-AUC and PR-AUC as main metrics
---------------------------------------------------------
Performance (example results)
Model	ROC-AUC	PR-AUC	F1 Score
Logistic Regression	0.78	0.41	0.52
Random Forest	0.85	0.52	0.61
LightGBM	0.87	0.55	0.64

LightGBM performed best, balancing interpretability and predictive power.

Feature importance & SHAP analysis revealed key churn drivers:

Low Tenure (newer customers churn more)

High Recency (long time since last order)

Complaints filed

Low SatisfactionScore

See detailed evaluation in notebooks/03_modeling.ipynb and 04_model_evaluation.ipynb.
------------------------------------------------
Interpretability & Insights

SHAP values confirm that recency, complaints, and satisfaction dominate churn risk.

Customers with frequent coupon usage and short tenure show mixed risk ‚Äî some are price-sensitive, others highly engaged.

Actionable strategies:

Early engagement campaigns for new customers

Fast complaint resolution workflows

Reward programs targeting high-recency but previously loyal users

Insights documented in notebooks/05_interpretability.ipynb.
-------------------------------------------------------
Project Structure
data/
  raw/          # original Excel (not versioned)
  processed/    # feature matrices
notebooks/      # step-by-step workflow
src/            # reusable Python modules
plots/          # interactive HTML charts (local only)
artifacts/      # saved pipelines & models
---------------------------------
Setup & Reproducibility
Conda
conda env create -f environment.yml
conda activate churn_env
jupyter lab

Requirements (pip alternative)
pip install -r requirements.txt
---------------------------------------
Key Takeaways

Churn rate ‚âà 16%: manageable imbalance.

Clear drivers: tenure, recency, complaints, satisfaction.

Tree-based models significantly outperform logistic regression.

SHAP provides transparency: actionable insights for retention strategies.

Pipeline ensures full reproducibility: same preprocessing for train & inference.
------------------------------------
Next Steps

Add time-based validation (simulate deployment).

Test uplift modeling for personalized retention campaigns.

Deploy pipeline as an API (FastAPI / Flask) for real-time scoring.

Automate retraining with fresh data.
-----------------------
üìú License

MIT License ‚Äì see LICENSE
.

üîó Author: Luiz Ot√°vio Marang√£o


































---------------------------------------------------------------------------------------------
# Churn Analysis

This project analyzes customer churn to identify patterns and predict customer attrition.

## Table of Contents

- [Overview](#overview)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Contributing](#contributing)
- [License](#license)

## Overview

Churn analysis helps businesses understand why customers leave and how to improve retention. This repository contains code and resources for data preprocessing, exploratory analysis, modeling, and evaluation.

## Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/churn_analysis.git
   cd churn_analysis
   ```
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

## Usage

1. Prepare your dataset in the `data/` directory.
2. Run the analysis scripts:
   ```bash
   python src/analyze.py
   ```
3. Review results in the `output/` directory.

## Project Structure

```
churn_analysis/
‚îú‚îÄ‚îÄ data/
‚îú‚îÄ‚îÄ src/
‚îú‚îÄ‚îÄ output/
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

## Contributing

Contributions are welcome! Please open issues or submit pull requests.

## License

This project is licensed under the MIT License.

---

‚úÖ Etapas j√° implementadas
Carregamento dos dados

Leitura da planilha ‚ÄúE Comm‚Äù e remo√ß√£o de colunas vazias.

Vis√£o geral inicial

Exibi√ß√£o do shape e primeiras linhas do DataFrame.

Mapa de duplicatas e valores faltantes (check de duplicated() e isnull()).

An√°lise univariada

Estat√≠sticas descritivas (mean, median, std, etc.) para colunas num√©ricas.

Distribui√ß√µes e contagens para vari√°veis categ√≥ricas.

An√°lise bivariada

Categ√≥ricas vs. Churn: histograms groupados por status de churn.

Num√©ricas vs. Churn: boxplots/violin plots para cada feature num√©rica.

Matriz de correla√ß√£o

C√°lculo e visualiza√ß√£o (heatmap ou gr√°fico interativo).

Insights sobre multicolinearidade e features correlacionadas ao target.

Detec√ß√£o de outliers & anomalias

IQR univariado (flag de outliers em cada vari√°vel).

Isolation Forest multivariado (flag e escore de anomalia).

Gr√°ficos interativos salvos em HTML.

üõ†Ô∏è Recomenda√ß√µes para completar a an√°lise
Distribui√ß√£o do target (Churn)

Gr√°fico de barras mostrando propor√ß√£o de clientes que churned vs. retained (avalia√ß√£o de balanceamento).

Mapa de missing values

Heatmap ou tabela de contagem de NaN por coluna, para identificar padr√µes de aus√™ncia de dados.

Verifica√ß√£o de tipos e convers√µes

Confirma√ß√£o de que datas, categorias e num√©ricos t√™m os tipos corretos; convers√£o de colunas quando necess√°rio.

Resumo estat√≠stico mais rico

Adi√ß√£o de m√©tricas de skewness, kurtosis e percentis (describe + extras) para detectar enviesamentos.

Distribui√ß√µes cont√≠nuas

KDE plots (ou histograma + curva) para features muito enviesadas (ex.: OrderCount, CashbackAmount).

Intera√ß√µes simples

Pairplot reduzido (2‚Äì3 features mais relevantes) para registrar rela√ß√µes n√£o lineares.

An√°lise de tempo

Se existir componente temporal (datas de pedido, tenure em dias), uma s√©rie temporal ou histograma sobre ‚Äúrecency‚Äù pode ajudar.

Consumo de mem√≥ria

Revisar uso de mem√≥ria do DataFrame e otimizar dtypes para deploy/pipeline.

---

O que j√° foi feito

Explora√ß√£o e pr√©‚Äëprocessamento dos dados

Carregamento e an√°lise explorat√≥ria de application_train.csv e application_test.csv.

Estudo das vari√°veis, identifica√ß√£o de colunas com alto percentual de aus√™ncias e documenta√ß√£o de todos os campos.

Cria√ß√£o de um pipeline de pr√©‚Äëprocessamento que:

Imputa ausentes (num√©ricos com mediana, categ√≥ricos com ‚ÄúUnknown‚Äù).

Escala num√©ricos com RobustScaler e codifica categorias com OneHotEncoder e TargetEncoder.

Garante aus√™ncia de vazamento de informa√ß√£o ao ser aplicado dentro da valida√ß√£o cruzada.

Persist√™ncia do objeto preprocessor.joblib e lista de colunas resultantes (train_columns.csv).

Desenvolvimento de modelos

Baseline com Logistic Regression L2 treinado em 5 folds estratificados; desempenho AUC ‚âà 0,746 e KS ‚âà 0,365.

Experimento com Elastic Net descartado por piora nas m√©tricas.

Migra√ß√£o para LightGBM com ajuste via Optuna; melhoria nas m√©tricas (AUC ‚âà 0,761, KS ‚âà 0,391, P@20 ‚âà 0,212).

Diagn√≥stico de calibra√ß√£o e aplica√ß√£o de regress√£o isot√¥nica nos 5 folds para calibrar as probabilidades (Brier caiu de 0,169 para 0,064 e a reliability curve se aproximou da diagonal
screenshot
).

Defini√ß√£o de um limiar de decis√£o por KS m√°ximo e cria√ß√£o de bandas de risco (Very Low, Low, ‚Ä¶, Very High) com base nas quantis das PDs calibradas; tudo salvo em thresholds_bands.json.

Interpretabilidade

C√°lculo de valores SHAP para o modelo LightGBM; gera√ß√£o de gr√°ficos de import√¢ncia global dos 20 principais atributos
screenshot
.

Prepara√ß√£o de estrutura para explica√ß√µes locais (gr√°ficos waterfall) em Streamlit.

Persist√™ncia de artefatos

Armazenamento de todos os componentes necess√°rios para scoring: preprocessor.joblib, lgbm_model.joblib, isotonic.joblib, feature_names_logreg.csv, thresholds_bands.json, imagens de SHAP e da curva de calibra√ß√£o.

Scripts, API e Dashboard

scripts/preprocess.py implementa fun√ß√µes para carregar artefatos e pontuar novos dados com atribui√ß√£o de bandas de risco.

api.py exp√µe um servi√ßo FastAPI com /score_one e /score_batch, retornando PD bruta, PD calibrada, banda de risco e decis√£o aprovar/declinar, al√©m de um endpoint de health check.

streamlit_app.py oferece um painel interativo no qual √© poss√≠vel carregar arquivos CSV para pontua√ß√£o em lote ou inserir manualmente dados de um cliente e visualizar a explica√ß√£o SHAP local (cascata).

üõ†Ô∏è O que pode melhorar para um projeto profissional

Organiza√ß√£o de pacotes: extrair as fun√ß√µes de scoring duplicadas no api.py e no streamlit_app.py para um m√≥dulo utilit√°rio √∫nico (por ex. scripts/preprocess.score_df), evitando diverg√™ncias na l√≥gica.

Script de treinamento: o arquivo train_model.py est√° vazio; seria interessante migrar as etapas de treinamento dos notebooks para um script parametrizado para facilitar re‚Äëtreinos e CI/CD.

Documenta√ß√£o:

Completar o README.md com uma descri√ß√£o clara do problema, fluxo de modelagem, instru√ß√µes de setup (como instalar depend√™ncias, rodar notebooks, API e Streamlit) e exemplos de uso.

Adicionar coment√°rios e docstrings nos scripts principais (preprocess.py, api.py, streamlit_app.py) para melhorar a legibilidade.

Valida√ß√£o extra:

Adicionar um conjunto de testes unit√°rios simples para verificar se a pipeline est√° carregando e se o API retorna os campos esperados.

Considerar m√©tricas adicionais (Precision@k, Recall@k) e algum tipo de an√°lise de fairness se houver requisitos legais.

Controle de vers√£o de dados: explicitar onde os dados brutos ficam armazenados (pasta data/raw) e n√£o version√°‚Äëlos no GitHub; usar .gitignore para arquivos grandes e sens√≠veis.

Ambiente de depend√™ncias: garantir que requirements.txt esteja atualizado e possua vers√µes testadas, al√©m de opcionalmente fornecer um environment.yml para quem preferir Conda.

Automa√ß√£o de deploy: criar scripts para subir a API e o Streamlit em um servi√ßo (Heroku, Render, etc.) ou mesmo Dockerizar a aplica√ß√£o para facilitar a execu√ß√£o em outras m√°quinas.

üì¶ Etapas para finalizar e subir ao GitHub

Limpar o reposit√≥rio:

Remover dados brutos sens√≠veis ou grandes; manter somente um ‚Äúsample‚Äù ou instru√ß√µes para download.

Eliminar arquivos tempor√°rios ou redundantes do Jupyter.

Completar train_model.py (ou renome√°‚Äëlo) com o pipeline completo de treinamento e salvamento de artefatos.

Refatorar o c√≥digo para centralizar a l√≥gica de pontua√ß√£o e calibra√ß√£o em um √∫nico m√≥dulo e importar essa fun√ß√£o no API e no Streamlit.

Atualizar o README.md com descri√ß√£o, instru√ß√µes de instala√ß√£o (pip install -r requirements.txt), como rodar notebooks, como executar a API (uvicorn app.api:app --reload) e como iniciar o painel (streamlit run app/streamlit_app.py).

Adicionar testes b√°sicos e um pytest.ini se desejar ir al√©m.

Fazer commit estruturado: criar reposit√≥rio no GitHub, adicionar todos os scripts e notebooks relevantes, documentar no hist√≥rico as principais etapas.

Publicar: push para GitHub, configurar reposit√≥rio como portf√≥lio p√∫blico e, se desejar, adicionar badges de build, documenta√ß√£o e link para demo online.

Com essas melhorias e passos finais, seu projeto de credit‚Äërisk estar√° pronto para ser apresentado como parte de seu portf√≥lio profissional.
